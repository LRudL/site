<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="EA ideas are discussed in a previous post series, but what does the EA movement/community/whatever actually look like in practice, where did it come from, and whose idea was it to give those philosophy nerds all that money?">
    <title>Effective Altruism in practice - Rudolf's Writing</title>
    
    <link rel="stylesheet" href="../css/style.css">
</head>
<body>
    
<header>
    <nav>
        <div class="site-name heading-font">Rudolf &mdash; <a href="../">home page</a></div>
    </nav>
</header>

    <div class="container">
        <div class="column left">
            <div class="sticky-div">
                
    
        <nav id="toc" class="table-of-contents">
            <hr class="toc-separator">
            <ul>
                
                    <li class="toc-h2">
                        <a href="#section-0">Summary</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-1">The philosophers</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-2">The transhumanists</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-3">More philosophers & EA gets a name</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-4">Charity evaluators and billionaires</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-5">Organisations</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-6">The Situation</a>
                    </li>
                
                    <li class="toc-h3">
                        <a href="#section-7">Demographics</a>
                    </li>
                
                    <li class="toc-h3">
                        <a href="#section-8">Culture</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-9">Axes & trends</a>
                    </li>
                
                    <li class="toc-h3">
                        <a href="#section-10">"Long-termism" vs "near-termism"</a>
                    </li>
                
                    <li class="toc-h3">
                        <a href="#section-11">Frugality vs spending</a>
                    </li>
                
                    <li class="toc-h3">
                        <a href="#section-12">Thinking vs doing</a>
                    </li>
                
                    <li class="toc-h2">
                        <a href="#section-13">Exciting Attempt for Enabling Action on Essential Activities</a>
                    </li>
                
            </ul>
            <hr class="toc-separator">
        </nav>
    

            </div>
            <div class="content"></div>
        </div>
        <div class="column main">
            <div class="collapsed-sidebar">
                
            </div>
            <div class="content">
                
    <article>
        <h2 class="post-title">Effective Altruism in practice</h2>
        <div class="post-meta">
            
            <p>
                <time datetime="2022-08-20">2022-08-20</time>
                &middot; 6.7k words (reading time: 24min)
            </p>
            <p class="post-summary"> <strong>Summary</strong>: EA ideas are discussed in a previous post series, but what does the EA movement/community/whatever actually look like in practice, where did it come from, and whose idea was it to give those philosophy nerds all that money?</p>
        </div>
        
        
            <div class="collapsed-sidebar">
                <nav id="collapsed-toc" class="table-of-contents">
                    <hr class="toc-separator">
                    <ul>
                        
                            <li class="toc-h2">
                                <a href="#section-0">Summary</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-1">The philosophers</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-2">The transhumanists</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-3">More philosophers & EA gets a name</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-4">Charity evaluators and billionaires</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-5">Organisations</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-6">The Situation</a>
                            </li>
                        
                            <li class="toc-h3">
                                <a href="#section-7">Demographics</a>
                            </li>
                        
                            <li class="toc-h3">
                                <a href="#section-8">Culture</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-9">Axes & trends</a>
                            </li>
                        
                            <li class="toc-h3">
                                <a href="#section-10">"Long-termism" vs "near-termism"</a>
                            </li>
                        
                            <li class="toc-h3">
                                <a href="#section-11">Frugality vs spending</a>
                            </li>
                        
                            <li class="toc-h3">
                                <a href="#section-12">Thinking vs doing</a>
                            </li>
                        
                            <li class="toc-h2">
                                <a href="#section-13">Exciting Attempt for Enabling Action on Essential Activities</a>
                            </li>
                        
                    </ul>
                    <hr class="toc-separator">
                </nav>
            </div>
        

        <hr>
        
        <p>I've written about <a href="https://www.strataoftheworld.com/2020/07/ea-ideas-1-rigour-and-opportunity-in.html">key ideas in Effective Altruism</a> before. But that was the theory. How did EA actually come to exist, and what does it look like in practice?</p>
<h2 id="section-0">Summary</h2>
<ul>
<li>The ideas underpinning EA came from many sources, including:</li>
<li>late-1900s analytic moral philosophers like Peter Singer and Derek Parfit;</li>
<li>futurist/transhumanist thinkers like Nick Bostrom and Eliezer Yudkowsky focusing on risks from future technologies;</li>
<li>a few people working on evaluating charity effectiveness;</li>
<li>efforts starting around 2010 by a few Oxford philosophers including William MacAskill and Toby Ord that, sometimes unwittingly, gave structure and a name to a diverse cluster of ideas about how to maximise your positive impact.</li>
<li>Though EA is <a href="https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology">framed around the question</a> of "what does the most good (according to an analytic and often quantiative framework based on impartial welfare-oriented ethics)?" rather than any particular answer to that question, in practice much (but not all!) EA efforts focus on one of the following, due to many people deciding that it's a particularly pressing and (outside EA) neglected problem:</li>
<li>reducing the risk of civilisation-wide catastrophe, especially from emerging technologies like advanced AI and biotechnology;</li>
<li>health and development in poor countries; and</li>
<li>animal welfare.</li>
<li>There is also a lot of work at the meta-level, including on figuring out how people can have <a href="https://80000hours.org/">impactful careers</a>, and trying to direct effort towards the above problems.</li>
<li>The funding for most EA-related projects and EA-endorsed charities comes from a combination of:</li>
<li>many individual small donors, in particular:<ul>
<li>people who have taken the <a href="https://www.givingwhatwecan.org/">Giving What We Can pledge</a> and therefore donate &gt;10% of their salary to highly effective charities;</li>
<li>people who explicitly pursue <a href="https://80000hours.org/articles/earning-to-give/">"earning-to-give"</a> (getting a high-paying job in order to donate most of the proceeds to charities);</li>
</ul>
</li>
<li>several foundations that derive their wealth from billionaires, including most prominently:<ul>
<li><a href="https://www.givingwhatwecan.org/">Open Philanthropy</a>, mostly funded by Dustin Moskovitz who made his wealth from being a Facebook co-founder; and</li>
<li><a href="https://ftxfoundation.org/">FTX Foundation</a>, funded by Sam Bankman-Fried and several other early employees at the crypto exchange FTX.</li>
</ul>
</li>
<li>There is no monolithic EA organisation (though the <a href="https://www.centreforeffectivealtruism.org/">Centre for Effective Altruism</a> organises some common things like the EA Global conferences), but rather a large collection of organisations that mainly share:</li>
<li>a commitment to maximising their positive impact on the world;</li>
<li>a generally rigorous and quantitative approach to doing so; and</li>
<li>some link to the cluster of people and organisations in Oxford that first named the idea of Effective Altruism.</li>
<li>There are also many charities that have no direct relation to the EA movement, but were identified by charity evaluators like <a href="https://www.givewell.org/">GiveWell</a> as extremely effective, and have thus been extensively funded.</li>
<li>EA is very good at attracting talented people, especially ambitious young people at top universities.</li>
<li>EA culture leans intellectual and open, and has a high emphasis on "epistemic rigour", i.e. being very careful about trying to figure out what is true, acknowledging and reasoning about uncertainties, etc.</li>
<li>Some "axes" within EA include:</li>
<li>"long-termists" who focus on possible grand futures of humanity and the existential risks that stand between us and those grand futures, and "near-termists" who work on clearer and more established things like global poverty and animal welfare;</li>
<li>a bunch of people and ideas all about frugality and efficient use of money, and another bunch of people and ideas about using the available funding to unblock opportunities for major impact; and</li>
<li>a historical tendency to be very good at attracting philosophy/research-type people who like wrestling with difficult abstract questions, versus a growing need to find entrepreneurial, operations, and policy people to actually do things in the real world. </li>
</ul>
<h2 id="section-1">The philosophers</h2>
<p>In the beginning (i.e. circa the 1970s, when <a href="https://en.wikipedia.org/wiki/Unix_time">time is widely known to have begun</a>), there were a bunch of philosophers doing interesting work. One of them was Peter Singer. Peter Singer proposed questions like this (paraphrasing, not quoting, and updated with recent numbers):</p>
<blockquote>
<p>Imagine you're wearing a $5000 suit and you walk past a child drowning in a lake. Do you jump into the lake and save the child, even though it ruins your suit?</p>
<p>If you answered yes to the above, then consider this: it is <a href="https://blog.givewell.org/2020/11/19/our-recommendations-for-giving-in-2020/">possible to save a child's life in the developing world for $5000</a>; what justification do you have for spending that money on the suit rather than saving the life?</p>
<p>The only difference between the two scenarios seems to be distance to the dying child (and method of death and etc. but ssshh); is that distance really morally significant?</p>
</blockquote>
<p>(He is also known for arguing in favour of animal rights and abortion rights.)</p>
<p>Derek Parfit is another. He is particularly famous for the book <em>Reasons and Persons</em>, in which he asks questions (paraphrasing again) like this:</p>
<blockquote>
<p>Is a moral harm done if you cause fewer people to exist in the future than otherwise might have? How should we reason about our responsibilities to future generations and non-existing people more generally?</p>
<p>Does there exist a number of people living mediocre (but still positive) lives such that this world is better than some smaller number of people living very good lives?</p>
</blockquote>
<p>(He also talks about problems in the philosophy of personal identity, and the contradictions in moral philosophies based on self-interest.)</p>
<h2 id="section-2">The transhumanists</h2>
<p>Then, largely separately and around the 1990s, there came the transhumanists ("transhumanism" is a wide-reaching umbrella term for humanist thinking about radical future technological change). Perhaps the most notable are Nick Bostrom and Eliezer Yudkowsky.</p>
<p>Nick Bostrom thought long and hard about many wacky-seeming things with potentially cosmic consequences. He popularised the simulation hypothesis (the idea that we might all be living in a computer simulation). He <a href="https://nickbostrom.com/fable/dragon">argues against death</a> (something I <a href="https://www.strataoftheworld.com/2021/10/death-is-bad.html">strongly agree with</a>). He did lots of work on anthropic reasoning, which is about the question of how we should update information we get about the state of the world when taking into account that we wouldn't exist unless the state of the world allowed it. This leads to <a href="https://en.wikipedia.org/wiki/Sleeping_Beauty_problem">some thought experiments</a> that I'd classify as infohazards because of their tendency to spark an unending discussion whenever they're described. Conveniently, he also coined the term "<a href="https://en.wikipedia.org/wiki/Information_hazard">infohazard</a>".</p>
<p>Most crucially for EA, though, Bostrom has worked on understanding existential risks, which are events that might destroy humanity or permanently and drastically reduce the capacity of humanity to achieve good outcomes in the future. In particular, he has worked on risks from advanced AI, which he boosted to popularity with the 2014 book <em>Superintelligence</em>.</p>
<p>Bostrom's style of argument is like a dry protein bar, leaning toward straightforward extrapolation of conclusions from premises, especially if the conclusions seem crazy but the premises seem self-evident. Sometimes, though, he does apply some literary flair to make <a href="https://nickbostrom.com/utopia">an important point</a>, and also <a href="https://nickbostrom.com/poetry/poetry">occasionally writes poetry</a>.</p>
<p>Eliezer Yudkowsky wanted to create a smarter-than-human AI as fast as possible, until he realised this might be a Bad Idea and said "<a href="https://www.lesswrong.com/posts/SwCwG9wZcAzQtckwx/that-tiny-note-of-discord">oops</a>" and switched to the problem of making sure any powerful AIs we create don't destroy human civilisation. He founded the Machine Intelligence Research Institute (MIRI) to find out the answer.</p>
<p>Yudkowsky also wrote a <a href="https://www.lesswrong.com/rationality">massive series of blog posts</a> to try to teach people about how to reason well (for example, he covers a lot of ground from the cognitive biases literature), and then went on to try to convey the same lessons in what become <a href="http://www.hpmor.com/">the most popular work of Harry Potter fanfiction of all time</a>. His writing and argument style tends toward flowing narratives that are usually both very readable and verbose (though quite hit-or-miss in whether you like it).</p>
<p>He has Opinions (note the capital). He is extremely pessimistic about the chances of solving the AI alignment problem.</p>
<p>Yudkowsky is affiliated much more strongly with the loose "Rationalist community" than with EA. This is a collection of online blogs that was sparked by Yudkowsky's writing, and later in particular also that of <a href="https://slatestarcodex.com/">Scott Alexander</a>, who has become internet-famous for his own reasons too. The central forum is <a href="https://www.lesswrong.com/">LessWrong</a>. The relation between EA and Rationalism is best described by a joke that students (well, at least physics students) are fond of, first made by Richard Feynman: "physics is to mathematics as sex is to masturbation". Both EA and Rationalism involve lots of discussion about far-ranging abstract ideas that (for a certain type of person) are hard to resist; one blogger says "[t]he experience of reading LessWrong for the first time was brain crack" and <a href="https://chanamessinger.com/blog/ea-as-nerdsniping">goes on to propose</a> that EA ideas are best-spread by <a href="https://xkcd.com/356/">nerd-sniping</a> (i.e. telling people about ideas they find so interesting that they literally can't help but think about them). Both EA and the Rationalists put an incredible amount of effort and weight on trying to reason well, avoid biases and fallacies, and being careful (and often quantitative) about uncertainties. However, EA is very much about applying those things to do good in the real world to real people, while the Rationalist vibe is sometimes one of indulging in theorising and practising good thinking for their own sake. (This is not necessarily a criticism - I had fun discussing Lisp syntax in the comments section of <a href="https://www.lesswrong.com/posts/GAqCiWJBttazYGsJR/review-structure-and-interpretation-of-computer-programs">the LessWrong version of my review of <em>Structure and Interpretation of Computer Programs</em></a>, even though arguing about parentheses isn't exactly going to save the world (or is it ... ?).)</p>
<p>(I should also note that on the specific topic of AI risk, the Rationalist community is extremely impact-oriented, likely due to founder effects - or perhaps because AI risk is the EA cause area that is most full of juicy technical puzzles and philosophical confusions.)</p>
<h2 id="section-3">More philosophers &amp; EA gets a name</h2>
<p>Brian Christian's <em>The Alignment Problem</em> mentions in chapter 9 some funny details about the sequence of events that lead to the first few EA-by-name organisations. In 2009, then-Oxford-philosophy-student Will MacAskill had an argument about vegetarianism while in a broom closet. Unlike most arguments about vegetarianism, and echoing the vibe of much future EA thinking, this one was on the meta-level; the debate was not whether factory farming is bad, but how we should deal with the moral uncertainty around whether or not factory farming is ethical. MacAskill eventually started talking with Toby Ord (though in a graveyard rather than a broom closet), another philosophy student interested in <a href="https://www.strataoftheworld.com/2020/07/ea-ideas-3-uncertainty.html">questions around moral uncertainty</a>.</p>
<p>Together with one other person, the two of them <a href="https://www.moraluncertainty.com/">wrote a book</a> on moral uncertainty. MacAskill and a philosophy-and-physics student called Benjamin Todd founded an organisation called <a href="https://80000hours.org/">80 000 Hours</a> to try to figure out how people can choose careers to have the greatest positive impact on the world. Toby Ord founded an organisation called <a href="https://www.givingwhatwecan.org/">Giving What We Can</a> (GWWC) that encourages people to donate 10% of their salary to exceptionally effective charities. GWWC estimates its roughly 8000 members have donated $277mn, and are likely to donate almost \$3bn over their lifetimes.</p>
<p>As an umbrella organisation for both of these, they created the <a href="https://www.centreforeffectivealtruism.org/">Centre for Effective Altruism</a>. Originally the "Effective Altruism" part was intended purely as a descriptive part of the organisation's name, but at some point started to stand more broadly for the general space of effectively altruistic things that at some point interacted with ideas from the original Oxford cluster.</p>
<p>Later, MacAskill wrote a book called <em>Doing Good Better</em> summarising ideas about why charity effectiveness is important and counterintuitive. Ord in turn wrote <a href="https://theprecipice.com/"><em>The Precipice</em></a> that summarises ideas about how mitigating existential risks to human civilisation is likely a key moral priority; after all, it would be bad if we all died.</p>
<h2 id="section-4">Charity evaluators and billionaires</h2>
<p>Independently from (and before) anything happening in Oxford broom closets, starting in 2006 hedge fund managers Holden Karnofsky and Elie Hassenfeld started thinking seriously about which charities to donate to. Upon discovering that this is a surprisingly hard problem, they started <a href="https://www.givewell.org/">GiveWell</a>, an organisation focused on finding exceptionally effective charities. They ended up concentrating on global health (their list includes malaria prevention, vitamin supplementation, and cash transfers, all in developing countries).</p>
<p>After a few years of GiveWell existing, they were put in touch with Dustin Moskovitz and Cari Tuna. At the time, Facebook co-founder Dustin Moskovitz was the world's youngest self-made billionaire, and with his partner Cari Tuna had started a philanthropic organisation called Good Ventures in 2011.</p>
<p>What followed was a cinematic failure of prioritisation, as recounted by Holden Karnofsky himself in <a href="https://80000hours.org/podcast/episodes/holden-karnofsky-most-important-century/#holdens-background-000947">this interview</a>. The GiveWell founders decided that "[meeting the billionaires] just doesn't seem very high priority", and thought that "[n]ext time someone's in California we should definitely take this meeting, but [...] this isn't the kind of thing we would rush for [...]". However, Karnofsky realised this meeting was an excellent excuse to go on a date with a Californian he fancied (and later married), and as a result ended up making the trip sooner rather than later.</p>
<p>Moskovitz and Tuna turned out to have very simplistic preferences for charitable giving: they just wanted to do the most good possible. This was an excellent fit with GiveWell's philosophy, and soon Good Ventures partnered with GiveWell in what would later become Open Philanthropy (of which Karnofsky would become co-CEO). <a href="https://www.openphilanthropy.org/">Open Philanthropy</a> is a key funder of EA projects, though they fund unrelated things as well (though always through a very EA lens of trying to rigorously and quantitatively maximise impact) . They list all their grants <a href="https://www.openphilanthropy.org/grants/">here</a>. </p>
<p>While studying physics at MIT, Sam Bankman-Fried (or "SBF"), already deeply interested in consequentialist moral philosophy, attended a talk by Will MacAskill on EA ideas. After stints at trading companies and the Centre for Effective Altruism, he founded the crypto-focused trading companies Alameda Research and then FTX, and ended up becoming the richest under-30 person in the world. (Though then the value of FTX fell in the crypto crash, and he recently turned 30 to boot.)</p>
<p><em>EDIT: In November 2022, both FTX and Alameda Research collapsed in a matter of days, and it became clear that FTX had committed major and flagrant financial fraud by transferring customer funds to Alameda, which Alameda then speculated with, and seems to have lost to the tune of billions of dollars. SBF is facing criminal charges. FTX and SBF have been condemned in harsh terms by those running many EA orgs and in countless EA Forum posts. Obviously, FTX and SBF have now very clearly become examples of what NOT to do. All of the following seem true: (a) our prior should be that people committing illegal and immoral actions that lead to extreme wealth and prestige for themselves are most likely acting mostly for the standard boring selfishly-evil reasons, (b) SBF probably had an easier time justifying his crimes because of the story that he could tell himself about doing good for the world, (c) publicly associating himself with EA, and receiving positive attention from EA organisations, helped make SBF appear moral and trustworthy, (d) there existed evidence and signals (in particular reports from Alameda's early days about cut-throat behaviour from SBF) that provided evidence of SBF's character before the FTX collapse, and (e) it is generally harder than it seems in hindsight to be right about whether a business is fraudulent (consider that coutless venture capitalists poured billions into FTX, and presumably had incentive to figure out if the entire thing was a scam). More information will come to light with time, and there are definitely lessons to be learned. Apart from this paragraph, I have not changed any part of this post.</em></p>
<p>SBF often emphasises that you're more likely to achieve outlier success in business if your goal is to donate the money effectively. There's little personal gain in going from \$100M to \$10B, so a selfish businessperson is likely to optimise something like "probability I earn more than [amount that lets me do whatever the hell I want for the rest of my life]", while a (mathematically-literate) altruistic one is far more compelled to simply shoot for the highest <a href="https://en.wikipedia.org/wiki/Expected_value">expected-value</a> outcomes, even if they're risky. (The exception is the selfish businessperson who really likes competing in the billionaire rankings.)</p>
<p>SBF has also said - and is living proof of - the idea that if your strategy to do good is to earn money to donate, you should probably aim for the risky but high-value bets (e.g. starting a company and becoming a billionaire), rather than going into some high-paying finance job earning a crazy-high but non-astronomical salary. Many people persuaded by EA ideas have done the latter, but SBF contributed more than all of them combined. The maths probably still works out even after accounting for the fact that SBF's route was far more unlikely to work than a finance job (he thought FTX had an 80% chance of failure). <a href="https://forum.effectivealtruism.org/posts/m35ZkrW8QFrKfAueT/an-update-in-favor-of-trying-to-make-tens-of-billions-of">This post</a> argues so. <a href="https://www.wave.com/en/about/">Wave</a>, a fintech-for-Africa company with strong EA representation in its founding team and a $1.7B valuation in 2021, is another example of EA business success.</p>
<p>SBF and other senior FTX people (many of who care deeply about EA ideas) launched the FTX Foundation, which in particular contains the <a href="https://ftxfuturefund.org/area-of-interest/">Future Fund</a> that has quickly become a key funder of the more future-oriented and speculative parts of EA.</p>
<p>These days, being associated with tech billionaires isn't a ringing endorsement. However, consider a few things. First, the tech billionaires aren't the ones who came up with the ideas or set the agendas. Sports car enthusiast and sci-fi nerd Elon Musk decided that sexy cars and rockets are the most important projects in the world and directed his wealth accordingly; Moskovitz, SBF, &amp; co. were persuaded by abstract arguments and donate their wealth to foundations where the selection of projects is done by people more knowledgeable in that than they are. Second, it seems unusually likely that the major EA donors really are sincere and committed to trying to do the most good; after all, if they wanted to maximise their popularity or acclaim, there are better ways of doing that then funding a loose cluster of people often trying to work specifically on the the least-popular charitable causes (since those are most likely to contain low-hanging fruit). Finally, if some tech billionaires endorsing EA is evidence <em>against</em> EA being a good thing, then no tech billionaires endorsing EA <a href="https://www.lesswrong.com/rationality/conservation-of-expected-evidence">must be</a> evidence <em>in favour</em> of EA being a good thing. However cynical you are about tech billionaires, they're still smart people, so a few of them going "huh, this is the type of thing I want to spend all my wealth on" should be more promising than all of them going "nope I don't buy this".</p>
<p>(If EA has some top tech business people, why doesn't it have some top political people too, or even funders from outside tech? My guess is a combination of factors. Politicians skew old while EAs skew young (partly because EA itself is young). Both EAs and tech people tend to be technically/mathematically/intellectually-inclined (though many areas within EA are specifically about social science or the humanities). Both EAs and tech people tend to care less than average about social norms or prestige, while politicians tend to be selected out of the set of people who are willing to optimise very hard for prestige and popularity. Also, expect some policy-related efforts from EA; many EAs work or aim to work in non-political policy roles, and there have even been some political efforts, though <a href="https://forum.effectivealtruism.org/posts/sKwEB7EEMaCp9tfaw/carrick-flynn-results-and-additional-ideas-for-passing">there is much to learn in that field</a>.)</p>
<h2 id="section-5">Organisations</h2>
<p>In addition to the previously-mentioned CEA, 80 000 Hours, Giving What We Can, GiveWell, Open Philanthropy, and FTX Foundation, organisations with a strong EA influence include (but are not limited to):</p>
<ul>
<li>A large number of think-tanks and research institutes, especially ones where people think about the end of the world all day, including</li>
<li><a href="https://www.fhi.ox.ac.uk/">Future of Humanity Institute</a> (FHI) at Oxford, which researches big-picture questions about the future of humanity and is run by Nick Bostrom.</li>
<li><a href="https://futureoflife.org/">Future of Life Institute</a> (FLI) in Cambridge (Massachusetts), focusing on global catastrophic risks and existential risks. It was founded by a team including Skype co-founder Jaan Tallinn and physicist Max Tegmark. Wikipedia says they are "[n]ot to be confused with Future of Humanity Institute" but to be honest this is a pretty big ask given the name.</li>
<li><a href="https://www.cser.ac.uk/">Centre for the Study of Existential Risk</a> (CSER) at Cambridge, also co-founded by Jaan Tallinn.</li>
<li><a href="https://longtermrisk.org/">Centre on Long-Term Risk</a> (CLR).</li>
<li><a href="https://www.longtermresilience.org/">Centre on Long-Term Resilience</a> (CLTR) (no, this is not confusing at all, it's all in your head).</li>
<li>A large number of animal welfare charities, which I won't bother listing, except to point out the meta-level <a href="https://animalcharityevaluators.org/">Animal Charity Evaluators</a>.</li>
<li>A large number of global health charities, including ones that are simply highly recommended (and funded) by GiveWell (in particular <a href="https://www.againstmalaria.com/">Against Malaria Foundation</a>, which routinely tops <a href="https://www.givewell.org/charities/top-charities">GiveWell rankings</a>) to ones that also trace their roots solidly to EA.</li>
<li>Organisations working on AI risk, including:</li>
<li><a href="https://www.anthropic.com/">Anthropic</a>, working on interpreting machine learning models (a program lead by Chris Olah) and more general empirically-grounded, engineering-based machine learning safety research.</li>
<li><a href="https://www.redwoodresearch.org/">Redwood Research</a>, a smaller company also doing empirical machine learning safety work (and running <a href="https://forum.effectivealtruism.org/posts/vvocfhQ7bcBR4FLBx/apply-to-the-second-ml-for-alignment-bootcamp-mlab-2-in">great ML bootcamps</a> on the side).</li>
<li><a href="https://humancompatible.ai/">Centre for Human-compatible AI</a> (CHAI), a research institute at UC Berkeley.</li>
<li><a href="https://intelligence.org/">Machine Intelligence Research Institute</a> (MIRI), the original AI safety organisation that was founded in 2000 and hence managed to snap up the enviable domain name "<a href="https://intelligence.org/">intelligence.org</a>". MIRI's research leans much more mathematical and theory-based than that of most other AI alignment organisations.</li>
<li><a href="https://www.conjecture.dev/">Conjecture</a>, a new organisation focusing on work that is relevant if advanced AI is surprisingly close.</li>
<li>(OpenAI and DeepMind, the two leading AI companies, both have safety teams that include people very committed to working on existential risk concerns. However, neither is primarily an AI safety company, and both weight advanced AI risks at a company-level less than the other companies on this list. OpenAI in particular sees AI risks more through the near-term lens of making sure AI systems and their benefits are widely accessible to everyone, rather than focusing on making sure AI systems don't doom us all (though I guess that too would be a suitably equitable outcome?).)</li>
<li><a href="https://www.alveavax.com/">Alvea</a>, a recent vaccine startup, with the eventual goal of enabling faster vaccine roll-out in the next pandemic. </li>
<li><a href="https://www.charityentrepreneurship.com/">Charity Entrepreneurship</a>, a charity incubator that has incubated <a href="https://www.charityentrepreneurship.com/our-charities">many charities</a>, including for example Healthier Hens (farmed chicken welfare), the Happier Lives Institute (helping policymakers figure out how to increase people's happiness), and Lead Exposure Elimination Project (working to reduce lead exposure in developing countries).</li>
<li><a href="https://www.sparkwave.tech/">SparkWave</a>, an incubator for software companies that are solving important problems. </li>
<li><a href="https://effectivethesis.org/">Effective Thesis</a>, trying to save students from writing pointless theses.</li>
<li><a href="https://founderspledge.com/">Founders Pledge</a>, which helps entrepreneurs commit to giving away money when they sell their companies and donate that money effectively (not to be confused with the more famous <a href="https://en.wikipedia.org/wiki/The_Giving_Pledge">Giving Pledge</a>). (So far, about $475mn has been donated in this way) </li>
<li><a href="https://www.legalpriorities.org/">Legal Priorities Project</a>, which looks at the legal aspects of trying to do everything else.</li>
<li><a href="https://allfed.info/">ALLFED</a> (ALLiance to Feed Earth in Disasters), which aims to be useful in situations where hundreds of millions of people or more are suddenly without food, and which has successfully found the best conceivable name for an organisation that does this.</li>
<li><a href="https://ourworldindata.org/">Our World in Data</a> (OWID), the world's best provider of data and graphs on important global issues. I'm not quite sure how interrelated they are with EA directly, but their founder <a href="https://forum.effectivealtruism.org/posts/uaveEAgFfyFx4EYaH/a-new-our-world-in-data-article-on-longtermism">posts on the EA Forum about OWID articles on very EA-related ideas</a>, so there's definitely some overlap.</li>
<li><a href="https://www.appgfuturegenerations.com/">All-Party Parliamentary Group for Future Generations</a> in the UK government.</li>
<li>A bunch of organisations focused on getting people interested in the world's biggest problems and teaching them various skills:</li>
<li><a href="https://www.atlasfellowship.org/">Atlas Fellowships</a>, a recent initiative for high-schoolers.</li>
<li>A collection of Existential Risk Initiatives running, among other things, summer internships where people (mostly undergraduate/postgraduate students) work with mentors on existential risk research: <a href="https://cisac.fsi.stanford.edu/stanford-existential-risks-initiative/content/stanford-existential-risks-initiative">SERI</a> (Stanford), <a href="https://effectivealtruism.ch/swiss-existential-risk-initiative">CHERI</a> (Switzerland), <a href="https://www.camxrisk.org/">CERI</a> (Cambridge), and a newer one at the University of Chicago which I can't yet find a website for, but which will almost certainly not help with the naming situation when it arrives. Thankfully, rumours say there will be soon be a YETI (Yale Existential Threats Initiative), which is a cool and (thank god!) unconfusable name.</li>
</ul>
<p>Since EA is not a monolithic centralised thing, there is plenty of fuzziness in what counts as an EA organisation, and definitely no official list (and therefore if you're reading this and your org is not on the list, you shouldn't complain - many great orgs were left out). The common features among many of them are:</p>
<ul>
<li>Some causal link to stuff that at some point interacted with the original Oxford cluster.</li>
<li>Emphasis on taking altruistic actions with a focus on effectiveness.</li>
<li>Emphasis on quantifying the impact of altruistic actions.</li>
<li>Emphasis on a scope that is in some way particularly wide-ranging or unconventional, either in sheer size or time (existential risks, the long-run future), geography (focusing on the entire world and often particularly developing countries rather than the organisation's neighbourhood), or in what is cared about (farmed animal welfare, <em>wild</em> animal welfare, the lives of people in the far future, and whatever the hell <a href="https://thequaliaresearchinstitute.org/">these people</a> are doing).</li>
</ul>
<p>The biggest EA events are the Effective Altruism Global (EAG) conferences organised by CEA. These usually happen several times a year, mostly in the UK and the Bay Area, though locally-organised <a href="https://www.eaglobal.org/eagxhome/">EAGx conferences</a> have more diverse locations. EAGs can be quite prolific - the 2015 EA Global Conference included Nick Bostrom (who I mentioned before), Stuart Russell (coauthor with Peter Norvig of the most popular AI textbook, and founder of CHAI), and Elon Musk (Elon Musk).</p>
<h2 id="section-6">The Situation</h2>
<p>EA has a strong presence especially at top universities. There are large and active EA student groups in the Bay Area, Cambridge, Oxford, and London, but also increasingly New York, Boston, and Berlin, and many smaller local groups (you can find them listed <a href="https://forum.effectivealtruism.org/community">here</a>). The profile of EA in the general public is very small. However, the concentration of talent is extremely high. Add to this the existence of funding bodies with tens of billions of dollars of assets that are firmly aligned with EA principles, and you can expect a lot of important, impactful work to come out of places with some connection to EA in the coming years.</p>
<p>It's important to keep in mind that EA is not a centralised thing. There is no EA tsar, or any single EA organisation that runs the show, or any official EA consensus. It's a cluster of many people and efforts that are joined mainly by caring about the types of ideas I talk about <a href="https://www.strataoftheworld.com/2020/07/ea-ideas-1-rigour-and-opportunity-in.html">here</a>.</p>
<h3 id="section-7">Demographics</h3>
<p><a href="https://effectivealtruismdata.com/#demographics">This website</a> has a good overview, based on whoever filled in a survey posted to the <a href="https://forum.effectivealtruism.org/">EA Forum</a>. The gender ratio is unfortunately somewhat skewed (70% male); for comparison, this is <a href="https://www.amacad.org/humanities-indicators/higher-education/gender-distribution-degrees-philosophy">roughly the same</a> as for philosophy degrees and better than for software developers (<a href="https://www.statista.com/statistics/1126823/worldwide-developer-gender/">90% male</a> (!?)). Half are 25-34. Over 70% are politically left or centre-left, and few are centre-right (2.5%) or right (1%), though almost 10% are libertarians. Education levels are high, and the five most common degrees are, in order: CS, maths, economics, social science, and philosophy. Most are from western countries.</p>
<h3 id="section-8">Culture</h3>
<p>EA culture places a lot of weight on epistemics: being honest about your uncertainties, clear about what would make you change your mind on an issue, aware of biases and fallacies, trying to avoid group-think, focusing on the substance of the issue rather than who said it or why, and arguing with the goal of finding the truth rather than defending your pet argument or cause. This is a lofty set of goals. To an astonishing but imperfect extent, and more so than any other concentration of people or writing (except from the equally-good Rationalist community mentioned above) that I've ever had any exposure to, EA succeeds at this.</p>
<p>Related to this, but also turbo-charged by general cultural memes of "critiquing cherished ideas is important", there's a high emphasis of constantly being on the lookout for ways in which you yourself or (in particular) common EA ideas might be wrong. If you read down the list of <a href="https://forum.effectivealtruism.org/allPosts?sortedBy=top&amp;timeframe=allTime&amp;filter=all">top-voted posts</a> on the EA Forum, they are about:</p>
<ol>
<li><a href="https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation">Potential failure modes resulting from the influx of money into EA.</a></li>
<li><a href="https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and">High EA spending being a problem for optics and epistemics.</a></li>
<li><a href="https://forum.effectivealtruism.org/posts/xomFCNXwNBeXtLq53/bad-omens-in-current-community-building">Things current EA community-building efforts are doing wrong, and why this is especially worrying.</a></li>
<li><a href="https://forum.effectivealtruism.org/posts/KDjEogAqWNTdddF9g/long-termism-vs-existential-risk">Reasons why some key concepts in EA are used misleadingly and unnecessarily.</a></li>
<li><a href="https://forum.effectivealtruism.org/posts/n3WwTz4dbktYwNQ2j/critiques-of-ea-that-i-want-to-read">A list of critiques of EA that someone wants expanded.</a></li>
<li><a href="https://forum.effectivealtruism.org/posts/QFa92ZKtGp7sckRTR/my-mistakes-on-the-path-to-impact">A catalogue of personal mistakes that someone made while trying to do good</a> (the key one being that they focused too much on working only at EA organisations).</li>
<li><a href="https://forum.effectivealtruism.org/posts/bsE5t6qhGC65fEpzN/growth-and-the-case-against-randomista-development">An argument that standard EA ways of trying to help with developing country development are not as effective as  other things.</a></li>
<li>And only in 8th place, something that isn't a critique of EA: <a href="https://forum.effectivealtruism.org/posts/cXBznkfoPJAjacFoT/are-you-really-in-a-race-the-cautionary-tales-of-szilard-and">a post about the historical case of early nuclear weapons researchers mistakenly assuming they were in a race, and implications for today's AI researchers</a></li>
</ol>
<p>(If you adjust upvotes on EA Forum posts to account for how active the forum was at the time, the most popular post of all time is <a href="https://forum.effectivealtruism.org/posts/FpjQMYQmS3rWewZ83/effective-altruism-is-a-question-not-an-ideology">Effective Altruism is a Question (not an ideology)</a>. It's not a critique, but it's also very revealing.)</p>
<p>Right now, there's <a href="https://forum.effectivealtruism.org/posts/8hvmvrgcxJJ2pYR4X/announcing-a-contest-ea-criticism-and-red-teaming">an active contest with $100k in prizes for the best critiques of EA</a>. This sort of stuff happens enough that Scott Alexander satirises it <a href="https://astralcodexten.substack.com/p/criticism-of-criticism-of-criticism">here</a>.</p>
<p>This might give the impression of EA as excessively-introspective and self-doubting. There is some truth to the introspectiveness part. However, the general EA attitude is also one of making bold (but reasoned) bets. Recall SBF's altruistically-motivated risk taking, or more generally the fact that <a href="https://www.openphilanthropy.org/research/hits-based-giving/">one of Open Philanthropy's foundational ideas</a> is to support reasonable-but-risky projects, or even more generally the way the whole thing is set up around unconventional and ambitious attempts at doing good.</p>
<p>If I had to name the two most important obstacles to doing important things in the real world, they would be (1) reasoning poorly and not updating enough based on feedback/evidence, and (2) being too risk-averse and insufficiently ambitious. Some cultures, like the good parts of academia, do well on avoiding (1). Others - imagine for example gung-ho Silicon Valley tech entrepreneurs - do well on avoiding (2). Though EA culture varies a lot between places and organisations, on the whole it seems uniquely good at combining these two aspects.</p>
<p>There are differences in culture between different EA hubs/clusters. I mainly have experience of the UK (and especially Cambridge) cluster and the Bay Area one. In the Bay, there is significant overlap between the EA and Rationalist communities, whereas in the UK there's mainly just EA in my experience. The Bay also leans more AI-focused and maybe weirder on average (or perhaps it's just a European vs American culture thing), while in the UK there are many AI-focused people but also many focused on biological fields (biosecurity &amp; alternative proteins) or policy.</p>
<h2 id="section-9">Axes &amp; trends</h2>
<h3 id="section-10">"Long-termism" vs "near-termism"</h3>
<p>In the history of EA, it's hard not to see an invasion of ideas from the planetary-scale futurism that people like Nick Bostrom and Eliezer Yudkowsky talked about, and Toby Ord (author of <em>The Precipice</em>) and Will MacAskill (about to drop <a href="https://www.whatweowethefuture.com/">a new book</a> on why we should prioritise the long-term future) increasingly focus on. Holden Karnofsky, who for a long time ran GiveWell, perhaps the most empirically-minded and global health -focused EA organisation, is now co-CEO of Open Philanthropy, responsible specifically for the speculative futurist parts of Open Philanthropy's mission, and <a href="https://www.cold-takes.com/the-most-important-century-in-a-nutshell/">writes blog posts about the grand future of humanity and why the coming century may be especially critical</a> (though he is careful to say that he doesn't think the other half of Open Philanthropy's work, or global health / animal welfare -focused charity more generally, is not important).</p>
<p>Perhaps this makes sense. In the long run at least, it seems sensible to expect the largest-scale ideas to be the most important ones. The rate of technological progress, especially in AI, has also been shrinking just what "the long run" means when expressed in years.</p>
<p>The common label applied to the ends of the radical-future-technology-focused versus concrete-current-problem-focused axis are "long-termist" and "near-termist" respectively. The name "long-termist" comes from arguments that the key moral priority is making sure we get to a secure, sustainable, and flourishing future civilisation (since such a civilisation could be very large and long-lasting, and therefore enable an enormous amount of happiness and flourishing). However, the names are a bit misleading. All existential risk work is often lumped into the long-termist category, so we have "long-termist" AI safety people trying to prevent a catastrophe many of them think will probably happen in the next three decades if it happens at all, and "near-termist" global health and development people trying to help the development of countries over a century.</p>
<p>(Many also <a href="https://forum.effectivealtruism.org/posts/rFpfW2ndHSX7ERWLH/simplify-ea-pitches-to-holy-shit-x-risk">point out</a> that caring about existential risks does not require the long-termist philosophy.)</p>
<h3 id="section-11">Frugality vs spending</h3>
<p>The culture of the original Oxford cluster was very frugal, and focused on monetary donations. For example, after founding Giving What We Can (GWWC), Toby Ord <a href="https://www.bbc.co.uk/news/magazine-11950843">donated everything he earned above £ 18 000 to charity</a> (and has <a href="https://www.vox.com/future-perfect/21728925/charity-10-percent-tithe-giving-what-we-can-toby-ord">continued on a similar track</a> since then). Because of the low available funding, the focus was very much on marginal impact - trying to figure out what existing opportunity could best use one extra dollar.</p>
<p>Since then, the arrival of billionaires meant that funding worries went down.</p>
<p>(For example, "earning to give" has gone down a lot in <a href="https://80000hours.org/career-reviews/#our-priority-paths">80 000 Hours' career rankings</a>. This is the idea that deliberately going into a high-earning job (often in finance) and then donating a significant fraction of your salary to top charities is one of the most effective ways to do good, and a path that many pursued based on the recommendation by 80 000 Hours.)</p>
<p>The bottleneck has moved (or at least been widely perceived to move) from funding to the time of people working on the key problems; instead of focusing on where to allocate the marginal dollar, the focus has somewhat shifted to how to allocate the marginal minute of time. In particular, the core argument of "imagine how far this particular dollar could go if used to effectively improve health in developing countries" has been joined by the argument of "there are plausible civilisation-ending disasters that could happen in the coming decades and require hard work to solve; imagine how sad it would be if we failed to work fast enough because we didn't spend that one dollar".</p>
<p>As a concrete example, Redwood Research organised <a href="https://www.alignmentforum.org/posts/YgpDYjTx7DCEgziG5/apply-to-the-ml-for-alignment-bootcamp-mlab-in-berkeley-jan">a machine learning bootcamp aimed at upskilling people for AI safety jobs</a> in January 2021 (and will be running more in the future, something I strongly endorse). Thirty participants (including myself) were flown into Berkeley from around the world, and spent three weeks living in a hotel while taking daily high-reliability COVID tests that I'm pretty sure weren't entirely free (and of course spending the days programming hard and talking about AI alignment (and eating free snack bars at the office - or maybe that last part was just me)). This wasn't cheap, nor was it a typical way to spend charity money (Redwood is <a href="https://www.openphilanthropy.org/grants/redwood-research-general-support/">funded</a> by Open Philanthropy). But if <a href="https://www.metaculus.com/questions/3479/date-weakly-general-ai-system-is-devised/">prediction markets are right that generally-capable AI starts emerging around the end of this decade</a>, and you take one look at the current state of progress on the AI alignment problem, and you do happen to have access to funding - well, it would be sad if being too stingy is how our civilisation failed.</p>
<p>Concretely, to look at only one consequence, Redwood made several hires from the bootcamp, despite the fact that many of the participants (myself included) were still students or otherwise not looking for work. Given how difficult but important hiring is, especially for high-skill technical roles, and the serious possibility that organisations like Redwood making progress is important for solving AI safety problems that might play a big role in how the future of humanity shapes out, this seems like a win.</p>
<p>However, at the same time, it is of course worth keeping in mind that humans are pretty good at thinking to themselves "man, wouldn't it be great if people like me had lots of money?" This, as well as the PR and culture problems of having lots of money sloshing around, are discussed in many EA Forum posts. We already saw that <a href="https://forum.effectivealtruism.org/posts/cfdnJ3sDbCSkShiSZ/ea-and-the-current-funding-situation">this one</a> (by MacAskill) and <a href="https://forum.effectivealtruism.org/posts/HWaH8tNdsgEwNZu8B/free-spending-ea-might-be-a-big-problem-for-optics-and">this one</a> are, respectively, the first- and second-most upvoted posts of all time on the EA Forum.</p>
<p>Ultimately, the whole point of Effective Altruism is, well, being effective about altruism. Whether EA funders spend quickly or slowly, and whichever causes they target, if they fail to find the best opportunities to do good with money, they haven't succeeded - and they know it.</p>
<p>(It should be noted that the GWWC criterion of donating 10% of your income to charity is met by many EAs, including ones far in space or culture from the original Oxford cluster, and global health is a leading donation target.)</p>
<h3 id="section-12">Thinking vs doing</h3>
<p>The fact that there's more resources - including not just funding but also the time of talented people - also means that the focus is less on marginal impact. If you have £10 and an hour, then figuring out what existing opportunity has the best ratio of good stuff per dollar is the best bet. But if you have, say, £10 000 000 and ten thousand work hours, then there's also the option of starting new projects and organisations.</p>
<p>(A lot of the weirdness of EA thinking comes from its marginalist nature. The things that are most valuable per marginal unit of money/time/effort are generally the things that are most neglected, and neglected things tend to seem weird because few people care about them. For example, the early EA focus basically completely eschewed developed country problems because per-dollar marginal cost-effectiveness was highest in poor countries; from the outside, this may look like a strangely harsh and idiosyncratic selection of causes. With increasing resources, it makes more sense to pursue larger-scale changes, and larger-scale changes sometimes look like more traditional and intuitive causes. For example, while developing country health and projects trying to improve the long-term future are Open Philanthropy's main focuses, they spend some of their massive budget on <a href="https://www.openphilanthropy.org/focus/criminal-justice-reform/">US criminal justice reform</a>, <a href="https://www.openphilanthropy.org/focus/land-use-reform/">land-use policy</a>, and <a href="https://forum.effectivealtruism.org/community">immigration policy</a>.)</p>
<p>Since EA now has the resources to start many new organisations, there's also starting to be a shift from EA being very research-oriented to having more and more real-world projects. Even though one of the key EA insights is that doing good requires lots of careful thinking in addition to good intentions and execution ability, the ultimate metric of success is actually improving the world, and that takes steps that aren't just research. I think EA has some headwind to overcome here; as a movement inspired, started, and (early on) largely consisting of philosophers, it has been remarkably successful in appealing to philosophical people and researchers, but not entrepreneurs or operations people to the same extent. I think it is a very welcome trend that this is starting to shift.</p>
<h2 id="section-13">Exciting Attempt for Enabling Action on Essential Activities</h2>
<p>EA is definitely not ideal, and it is also not guaranteed to survive. Like any real-world community, it is not a timeless platonic ideal of pure perfection that burst into the world fully formed, but rather something with an idiosyncratic history, that consists of real people, and has certain biases and cultural oddities. Still, I think it is probably the most exciting and useful thing in the world to be engaged with.</p>

        <hr>
        <div class="footer">
            
            
            
        </div>
    </article>

            </div>
        </div>
        <!--<div class="column right">
            <div class="sticky-div">
                
            </div>
            <div class="content"></div>
        </div>-->
    </div>
    <hr>
    <footer></footer>
</body>
</html>